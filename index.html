<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bastian Bunzeck" />
  <title>Learning language(s)</title>
  <link rel="stylesheet" href="reset.css" />
  <link rel="stylesheet" href="index.css" />
</head>
<body>
<table class="header">
  <tr>
    <td colspan="2" rowspan="2" class="width-auto">
      <h1 class="title">Learning language(s)</h1>
      <span class="subtitle">How humans do and machines can do
too.</span>
    </td>
    <th>Version</th>
    <td class="width-min">ðŸ”®.2</td>
  </tr>
  <tr>
    <th>Updated</th>
    <td class="width-min"><time style="white-space: pre;">Jun
â€™25</time></td>
  </tr>
  <tr>
    <th class="width-min">Author</th>
    <td class="width-auto"><a href="https://bbunzeck.github.io"><cite>Bastian
Bunzeck</cite></a></td>
    <th class="width-min">License</th>
    <td>MIT</td>
  </tr>
</table>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Contents</h2>
<ul class="incremental">
<li><a href="#about-me" id="toc-about-me">About me</a></li>
<li><a href="#publications" id="toc-publications">Publications</a></li>
<li><a href="#talks-and-presentations"
id="toc-talks-and-presentations">Talks and presentations</a></li>
<li><a href="#teaching" id="toc-teaching">Teaching</a></li>
</ul>
</nav>
<h2 id="about-me">About me</h2>
<p>Hi! My name is Bastian Bunzeck and I am a second year PhD student at
<a href="https://www.uni-bielefeld.de/">Bielefeld University</a>. I work
in the Computational Linguistics group (<a
href="https://clause-bielefeld.github.io/">CLAUSE</a>) under the
supervision of <a href="https://sinazarriess.github.io/">Prof.Â Sina
ZarrieÃŸ</a>. I am also a member of the collaborative research center <a
href="https://www.uni-bielefeld.de/sfb/sfb1646/">(CRC) 1646 â€“
<strong>L</strong>inguistic <strong>C</strong>reativity in
<strong>C</strong>ommunication</a> in Bielefeld. Before my PhD, I
studied English/American Studies and Computer Science at <a
href="https://www.uni-jena.de/">Friedrich Schiller University Jena</a>
in Germany and <a href="https://www.kuleuven.be/kuleuven/">Katholieke
Universiteit Leuven</a> in Belgium. In Jena I helped to develop the
corpus annotation tool <a
href="https://hexatomic.github.io/">Hexatomic</a> and also worked at the
<a href="https://www.iaa.uni-jena.de/">English department</a>.</p>
<p>I am interested in the relationship between (especially usage-based
and cognitive approaches to) linguistics on the one hand, and natural
language processing on the other hand. The neural turn in NLP has
realized many ideas already proposed much earlier in the literature on
connectionist modelling. Yet, it remains elusive how well
state-of-the-art models and the cognitive/linguistic reality actually
map to one another. In my research, I explore the ways in which
linguistic knowledge emerges in human language learners and neural
language models â€“ mostly from a usage-based and constructionist
perspective. Currently, my research focus in this direction lies on very
small language models trained with small amounts of data, and their
comparability to child language development â€“ lately also from a
multilingual perspective!</p>
<p>If you are looking for ways to contact me, check out <a
href="https://ekvv.uni-bielefeld.de/pers_publ/publ/PersonDetail.jsp?personId=419963705">my
page in the Bielefeld University staff directory</a> or send me an email
(firstname.lastname@uni-bielefeld.de).</p>
<hr>
<h2 id="publications">Publications</h2>
<p>For up-to-date overviews also check: <a
href="https://scholar.google.de/citations?user=yALZ_7kAAAAJ&amp;hl">Google
Scholar</a>, <a href="https://pub.uni-bielefeld.de/person/419963705">PUB
- Publications at Bielefeld University</a> and my <a
href="https://orcid.org/0000-0002-1832-4068">ORCID page</a>.</p>
<h4 id="preprints">Preprints</h4>
<ul class="incremental">
<li><strong>Bastian Bunzeck</strong>, Daniel Duran and Sina ZarrieÃŸ.
2025. Do Construction Distributions Shape Formal Language Learning In
German BabyLMs? <a href="https://arxiv.org/abs/2503.11593"
class="uri">https://arxiv.org/abs/2503.11593</a> (Conditionally accepted
at CoNLL 2025)</li>
<li><strong>Bastian Bunzeck</strong> and Sina ZarrieÃŸ. 2025. Subword
models struggle with word learning, but surprisal hides it. <a
href="https://arxiv.org/abs/2502.12835"
class="uri">https://arxiv.org/abs/2502.12835</a> (Accepted at ACL 2025
Main)</li>
</ul>
<h4 id="conferenceworkshop-papers">Conference/Workshop Papers</h4>
<ul class="incremental">
<li><strong>Bastian Bunzeck</strong>, Daniel Duran, Leonie Schade, and
Sina ZarrieÃŸ. 2025. Small language models also work with small
vocabularies: Probing the linguistic abilities of grapheme- and
phoneme-based baby llamas. In <em>Proceedings of the 31st International
Conference on Computational Linguistics</em>, pages 6039â€“6048, Abu
Dhabi, UAE. Association for Computational Linguistics. <a
href="https://aclanthology.org/2025.coling-main.404/"
class="uri">https://aclanthology.org/2025.coling-main.404/</a></li>
<li><strong>Bastian Bunzeck</strong>, Daniel Duran, Leonie Schade, and
Sina ZarrieÃŸ. 2024. Graphemes vs.Â phonemes: battling it out in
character-based language models. In <em>The 2nd BabyLM Challenge at the
28th Conference on Computational Natural Language Learning</em>, pages
54â€“64, Miami, FL, USA. Association for Computational Linguistics. <a
href="https://aclanthology.org/2024.conll-babylm.5/"
class="uri">https://aclanthology.org/2024.conll-babylm.5/</a></li>
<li><strong>Bastian Bunzeck</strong> and Sina ZarrieÃŸ. 2024. The SlayQA
benchmark of social reasoning: Testing gender-inclusive generalization
with neopronouns. In <em>Proceedings of the 2nd GenBench Workshop on
Generalisation (Benchmarking) in NLP</em>, pages 42â€“53, Miami, Florida,
USA. Association for Computational Linguistics. <a
href="https://aclanthology.org/2024.genbench-1.3/"
class="uri">https://aclanthology.org/2024.genbench-1.3/</a></li>
<li><strong>Bastian Bunzeck</strong> and Sina ZarrieÃŸ. 2024. Fifty
shapes of BLiMP: Syntactic learning curves in language models are not
uniform, but sometimes unruly. In <em>Proceedings of the 2024 CLASP
Conference on Multimodality and Interaction in Language Learning</em>,
pages 39â€“55, Gothenburg, Sweden. Association for Computational
Linguistics. <a href="https://aclanthology.org/2024.clasp-1.7/"
class="uri">https://aclanthology.org/2024.clasp-1.7/</a></li>
<li><strong>Bastian Bunzeck</strong> and Sina ZarrieÃŸ. 2023. GPT-wee:
How small can a small language model really get? In <em>Proceedings of
the BabyLM Challenge at the 27th Conference on Computational Natural
Language Learning</em>, pages 7â€“18, Singapore. Association for
Computational Linguistics. <a
href="https://aclanthology.org/2023.conll-babylm.2/"
class="uri">https://aclanthology.org/2023.conll-babylm.2/</a></li>
<li><strong>Bastian Bunzeck</strong> and Sina ZarrieÃŸ. 2023.
Entrenchment matters: Investigating positional and constructional
sensitivity in small and large language models. In <em>Proceedings of
the 2023 CLASP Conference on Learning with Small Data (LSD)</em>, pages
25â€“37, Gothenburg, Sweden. Association for Computational Linguistics. <a
href="https://aclanthology.org/2023.clasp-1.3"
class="uri">https://aclanthology.org/2023.clasp-1.3</a></li>
</ul>
<h4 id="journal-papers">Journal papers</h4>
<ul class="incremental">
<li><strong>Bastian Bunzeck</strong> and Holger Diessel. 2024. The
richness of the stimulus: Constructional variation and development in
child-directed speech. <em>First Language</em>. <a
href="https://doi.org/10.1177/01427237241303225"
class="uri">https://doi.org/10.1177/01427237241303225</a></li>
<li>Paula Wojcik, <strong>Bastian Bunzeck</strong>, and Sina ZarrieÃŸ.
2023. The Wikipedia Republic of Literary Characters. <em>Journal of
Cultural Analytics</em>, 8(2). <a
href="https://doi.org/10.22148/001c.70251%3C"
class="uri">https://doi.org/10.22148/001c.70251&lt;</a></li>
<li>Stephan Druskat, Thomas Krause, Clara Lachenmaier, and
<strong>Bastian Bunzeck</strong>. 2023. Hexatomic: An extensible,
OS-independent platform for deep multi-layer linguistic annotation of
corpora. <em>Journal of Open Source Software</em>, 8(86):4825. <a
href="https://doi.org/10.21105/joss.04825"
class="uri">https://doi.org/10.21105/joss.04825</a></li>
</ul>
<hr>
<h2 id="talks-and-presentations">Talks and presentations</h2>
<h4 id="section">2025</h4>
<ul class="incremental">
<li><em>What LLMs can do for linguisticsâ€¦and what linguistics can do for
LLMs</em>, (invited guest lecture, undergrad course on empirical
linguistics), Heinrich Heine UniversitÃ¤t DÃ¼sseldorf (Germany)<br />
</li>
<li><em>Word learning in (all kinds of) German and English BabyLMs</em>,
(poster presentation), HumanCLAIM Workshop, University of GÃ¶ttingen
(Germany)</li>
</ul>
<h4 id="section-1">2024</h4>
<ul class="incremental">
<li><em>Fifty shapes of BLiMP: syntactic learning curves in language
models are not uniform, but sometimes unruly</em>, (non-archival poster
presentation), BlackboxNLP 2024 at EMNLP 2024, Miami/Florida (US)</li>
<li><em>Constructions in child-directed speech</em> (with Holger
Diessel), (peer-reviewed oral presentation), 10th International
Conference of the German Cognitive Linguistics Association, OsnabrÃ¼ck
University (Germany)</li>
<li><em>Generating authentic child speech from little data</em>, (poster
presentation), NLG in the Lowlands 2024, Bielefeld University
(Germany)</li>
</ul>
<h4 id="section-2">2023</h4>
<ul class="incremental">
<li><em>GPT-wee: Experiments in downscaling and curriculum
learning</em>, (poster presentation), SAIL Workshop on Fundamental
Limits of Large Language Models, Bielefeld University (Germany)</li>
<li><em>From Byte to Babel: Large Language Models and the Tower of
Linguistic Knowledge</em>, (peer-reviewed oral presentation), META-LING
2023 - Methodological Exploration and Technological Advances in
Linguistics, University of Bamberg (Germany)</li>
<li><em>Where and How Do Literary Characters Figure in Wikipedia?</em>
(with Sina ZarrieÃŸ), (invited presentation), International Workshop |
Wikipedia, Wikidata and Wikibase: Usage Scenarios for Literary Studies,
Free University of Berlin (Germany)</li>
</ul>
<hr>
<h2 id="teaching">Teaching</h2>
<h4 id="summer-term-2025">Summer term 2025</h4>
<ul class="incremental">
<li>Neural nets in language technology â€“ seminar (taught in
English)</li>
</ul>
<h4 id="winter-term-20242025">Winter term 2024/2025</h4>
<ul class="incremental">
<li>Introduction to computational linguistics <em>(EinfÃ¼hrung in die
Computerlinguistik)</em> â€“ practical sessions, accompanying lectures by
Sina ZarrieÃŸ</li>
<li>Methods of applied computational linguistics <em>(Methoden der
angewandten Computerlinguistik)</em> â€“ lectures and practical
sessions</li>
</ul>
<h4 id="summer-term-2024">Summer term 2024</h4>
<ul class="incremental">
<li>Methods of applied computational linguistics <em>(Methoden der
angewandten Computerlinguistik)</em> â€“ lectures and practical
sessions</li>
<li>Neural nets in language technology <em>(Neuronale Netze in der
Sprachverarbeitung)</em> â€“ practical sessions, accompanying lectures by
Sina ZarrieÃŸ</li>
</ul>
<h4 id="winter-term-20232024">Winter term 2023/2024</h4>
<ul class="incremental">
<li>Introduction to computational linguistics <em>(EinfÃ¼hrung in die
Computerlinguistik)</em> â€“ practical sessions, accompanying lectures by
Sina ZarrieÃŸ</li>
<li>Project seminar: Modeling and analysis of dialogue
<em>(Projektseminar: Modellierung und Analyse von sprachlichen
Dialogen)</em>, taught jointly with Simeon SchÃ¼z</li>
</ul>
<h4 id="summer-term-2023">Summer term 2023</h4>
<ul class="incremental">
<li>Methods of applied computational linguistics <em>(Methoden der
angewandten Computerlinguistik)</em> â€“ practical sessions, accompanying
lectures by Sina ZarrieÃŸ</li>
</ul>
<hr>
<p>Design adapted from Oskar WickstrÃ¶mâ€™s <a
href="https://github.com/owickstrom/the-monospace-web">The Monospace
Web</a></p>
  <div class="debug-grid"></div>
  <script src="index.js"></script>
</body>
</html>
